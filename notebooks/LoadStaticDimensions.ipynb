{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b366bc6d-4162-4e7e-baeb-77c709e6f7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDCC5 LoadStaticDimensions - Dates and Times\n",
    "\n",
    "This notebook loads and transforms the **static dimensions** (`dates` and `times`) through the Medallion architecture:\n",
    "\n",
    "- **Raw → Bronze**: CSV files are read from the Raw layer and stored as Delta tables in Bronze.  \n",
    "- **Bronze → Silver**: typing transformations (`cast`) are applied using dedicated functions (`silver_transform_dates`, `silver_transform_times`).  \n",
    "- **Silver → Gold**: the final dimensional tables (`DimDates`, `DimTimes`) are created and made ready for the analytical model.  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13d9eed-9a24-497b-929a-eb7a59926fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import DateType, IntegerType, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b158d9-b2ce-4595-bf9a-35e1c08ba458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- `src_values`: list of static dimensions to process (`dates`, `times`).  \n",
    "- `raw_base_path`, `bronze_base_path`, `silver_base_path`, `gold_base_path`: base paths for each layer in the Medallion architecture.  \n",
    "- `pk_map`: maps each dimension to its primary key (`date_sk`, `time_sk`).  \n",
    "- `bronze_schema`, `silver_schema`, `gold_schema`: schemas where the tables will be stored in each layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c4cf3d-7e0d-40a5-885b-0505a4e42baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "src_values = [\"dates\", \"times\"]  \n",
    "raw_base_path = \"/Volumes/workspace/raw/rawvolume/rawdata\"\n",
    "bronze_base_path = \"/Volumes/workspace/bronze/bronzevolume\"\n",
    "silver_base_path = \"/Volumes/workspace/silver\"\n",
    "gold_base_path = \"/Volumes/workspace/gold\"\n",
    "pk_map = {\n",
    "    \"dates\": \"date_sk\",\n",
    "    \"times\": \"time_sk\"\n",
    "}\n",
    "bronze_schema = \"workspace.bronze\"\n",
    "silver_schema = \"workspace.silver\"\n",
    "gold_schema   = \"workspace.gold\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a6b1bd-304a-4085-ad04-3afd4797059b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **`silver_transform_dates(df)`**: Converts dates and related columns to proper types (date and int).\n",
    "\n",
    "- **`silver_transform_times(df)`**: Converts time keys to int and keeps time attributes as strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b8787e-52ad-437c-bdbb-6b468802d270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def silver_transform_dates(df):\n",
    "    return (df.withColumn(\"full_date\", F.col(\"full_date\").cast(DateType()))\n",
    "              .withColumn(\"date_sk\", F.col(\"date_sk\").cast(IntegerType()))\n",
    "              .withColumn(\"year\", F.col(\"year\").cast(IntegerType()))\n",
    "              .withColumn(\"month\", F.col(\"month\").cast(IntegerType()))\n",
    "              .withColumn(\"day\", F.col(\"day\").cast(IntegerType()))\n",
    "              .withColumn(\"weekday\", F.col(\"weekday\").cast(IntegerType()))\n",
    "              .withColumn(\"quarter\", F.col(\"quarter\").cast(IntegerType())))\n",
    "\n",
    "def silver_transform_times(df):\n",
    "    return (df.withColumn(\"time_sk\", F.col(\"time_sk\").cast(IntegerType()))\n",
    "              .withColumn(\"full_time\", F.col(\"full_time\").cast(StringType()))\n",
    "              .withColumn(\"hour\", F.col(\"hour\").cast(IntegerType()))\n",
    "              .withColumn(\"minute\", F.col(\"minute\").cast(IntegerType()))\n",
    "              .withColumn(\"day_part\", F.col(\"day_part\").cast(StringType()))\n",
    "              .withColumn(\"full_time_ampm\", F.col(\"full_time_ampm\").cast(StringType())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73fb5a1a-d134-449c-b653-6ff683378832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "For each dimension in `src_values`, the **Raw → Bronze → Silver → Gold** flow is executed:\n",
    "\n",
    "1. **Raw → Bronze**  \n",
    "   - Reads CSV files from the Raw layer.  \n",
    "   - Deletes the previous directory in Bronze and overwrites it with Delta-formatted data.  \n",
    "\n",
    "2. **Bronze → Silver**  \n",
    "   - Reads the Bronze table.  \n",
    "   - Applies the corresponding transformation function (`silver_transform_dates` or `silver_transform_times`).  \n",
    "   - Saves the result as a table in the Silver schema (`silver_{dim}`).  \n",
    "\n",
    "3. **Silver → Gold**  \n",
    "   - Takes the Silver table.  \n",
    "   - Deletes the previous Gold table if it exists.  \n",
    "   - Creates the final dimensional table (`DimDates`, `DimTimes`) in the Gold schema.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15024c3-a64b-497b-917c-87e74b0dca27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n===== Procesando DATES =====\n\n===== Procesando TIMES =====\n"
     ]
    }
   ],
   "source": [
    "for dim in src_values:\n",
    "    print(f\"\\n===== Procesando {dim.upper()} =====\")\n",
    "\n",
    "    pk_col = pk_map[dim]\n",
    "    raw_path = f\"{raw_base_path}/{dim}\"\n",
    "    bronze_path = f\"{bronze_base_path}/{dim}\"\n",
    "\n",
    "    raw_df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .load(raw_path)\n",
    "    )\n",
    "\n",
    "    ############################# BRONZE ######################\n",
    "    dbutils.fs.rm(bronze_path, recurse=True)\n",
    "    raw_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
    "\n",
    "    ############################# SILVER ########################\n",
    "    silver_table = f\"{silver_schema}.silver_{dim}\"\n",
    "    bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "    if dim == \"dates\":\n",
    "        bronze_df = silver_transform_dates(bronze_df)\n",
    "    elif dim == \"times\":\n",
    "        bronze_df = silver_transform_times(bronze_df)\n",
    "\n",
    "    bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "\n",
    "    ############################# GOLD ###################################\n",
    "    gold_table = f\"{gold_schema}.Dim{dim}\"\n",
    "    silver_df = spark.table(silver_table)\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {gold_table}\")\n",
    "    silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(gold_table)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5965413566616719,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "LoadStaticDimensions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
